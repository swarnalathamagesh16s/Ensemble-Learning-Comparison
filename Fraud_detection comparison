import pandas as pd
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix

# 1. Generate "Fake" Credit Card Data
# 1,000 transactions, 10 features (time, amount, location, etc.)
X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, 
                           n_redundant=5, weights=[0.9], random_state=42)

# Split into 80% Training and 20% Testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 2. Strategy A: Random Forest (The "Crowd" approach)
rf_model = RandomForestClassifier(n_estimators=100)
rf_model.fit(X_train, y_train)
rf_preds = rf_model.predict(X_test)

# 3. Strategy B: XGBoost (The "Learning from Mistakes" approach)
xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1)
xgb_model.fit(X_train, y_train)
xgb_preds = xgb_model.predict(X_test)

# 4. The Results
print("--- RANDOM FOREST RESULTS ---")
print(classification_report(y_test, rf_preds))

print("\n--- XGBOOST RESULTS ---")
print(classification_report(y_test, xgb_preds))
from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier

# 1. Define the "Grid" of settings we want to try
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.1, 0.01],
    'n_estimators': [50, 100, 200]
}

# 2. Initialize the search robot
# 'cv=3' means it will test each combo 3 different ways to be sure (Cross-Validation)
grid_search = GridSearchCV(XGBClassifier(), param_grid, cv=3, scoring='accuracy')

# 3. Run the search (this might take a few seconds)
grid_search.fit(X_train, y_train)

# 4. See the winning settings!
print(f"Best Settings: {grid_search.best_params_}")
print(f"Best Score: {grid_search.best_score_ * 100:.2f}%")

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# 1. Generate the Confusion Matrix data
# (Assuming 'y_test' are your true labels and 'xgb_preds' are your XGBoost guesses)
cm = confusion_matrix(y_test, xgb_preds)

# 2. Create the Heatmap using Seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Legit', 'Fraud'], 
            yticklabels=['Legit', 'Fraud'])

# 3. Add Labels
plt.xlabel('Predicted Label (What the model guessed)')
plt.ylabel('True Label (The actual truth)')
plt.title('XGBoost Confusion Matrix: Fraud Detection')
plt.show()
